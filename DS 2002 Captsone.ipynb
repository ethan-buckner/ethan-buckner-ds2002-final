{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5ed951df-567b-4067-ae8b-182896003da0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Ethan Buckner - Final Project\n",
    "\n",
    "I used the sample database ‘sakila’, provided by MySQL.\n",
    "\n",
    "We start by establishing our credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ba2b1e80-7ebd-49f1-a846-2c7ea016b6af",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001B[0m\nCollecting sqlalchemy\n  Downloading SQLAlchemy-2.0.13-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.7 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.7/2.7 MB 36.8 MB/s eta 0:00:00\nCollecting greenlet!=0.4.17\n  Downloading greenlet-2.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (613 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 613.7/613.7 kB 51.0 MB/s eta 0:00:00\nRequirement already satisfied: typing-extensions>=4.2.0 in /databricks/python3/lib/python3.10/site-packages (from sqlalchemy) (4.3.0)\nInstalling collected packages: greenlet, sqlalchemy\nSuccessfully installed greenlet-2.0.2 sqlalchemy-2.0.13\n\u001B[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "%pip install sqlalchemy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "03bc19df-91ac-4c3c-b31e-3851f56e1ad7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pymysql.cursors\n",
    "import pymongo\n",
    "from sqlalchemy import create_engine, text\n",
    "\n",
    "# Azure MySQL Server Connection Information ###################\n",
    "jdbc_hostname = \"ds2002ethanbuckner.mysql.database.azure.com\"\n",
    "jdbc_port = 3306\n",
    "src_database = \"sakila\"\n",
    "\n",
    "connection_properties = {\n",
    "  \"user\" : \"ethan_admin\",\n",
    "  \"password\" : \"King in the north!\",\n",
    "  \"driver\" : \"org.mariadb.jdbc.Driver\"\n",
    "}\n",
    "\n",
    "mysql_user_id = \"ethan_admin\"\n",
    "mysql_pwd = \"King in the north!\"\n",
    "mysql_host_name = \"ds2002ethanbuckner.mysql.database.azure.com\"\n",
    "\n",
    "atlas_cluster_name = \"EthanCluster\"\n",
    "atlas_user_name = \"root\"\n",
    "atlas_password = \"kinginthenorth\"\n",
    "\n",
    "conn_str = {\"local\" : f\"mongodb://localhost:27017/\",\n",
    "    \"atlas\" : \"mongodb+srv://root:kinginthenorth@ethancluster.tnalhre.mongodb.net/?retryWrites=true&w=majority\"\n",
    "}\n",
    "\n",
    "src_dbname = \"sakila\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "78500b73-27a3-402a-9778-a2d8e442b59c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr></tr></thead><tbody></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "dataframeName": null
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "CREATE DATABASE IF NOT EXISTS sakila_fact;\n",
    "\n",
    "USE sakila_fact"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6eee3d6a-3064-43d7-a267-2166e25d28f1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "I decided to drop the column ‘last_update’ from all tables because the data was not necessary and the timestamp data type would be difficult to work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "52a7f4c0-74e9-446b-bf09-50d3f65210a6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "conn = pymysql.connect(host=jdbc_hostname, user=\"ethan_admin\", password=\"King in the north!\", database=src_dbname)\n",
    "cursor = conn.cursor(pymysql.cursors.DictCursor)\n",
    "cursor.execute(\"ALTER TABLE actor DROP COLUMN last_update;\")\n",
    "cursor.execute(\"ALTER TABLE address DROP COLUMN last_update;\")\n",
    "cursor.execute(\"ALTER TABLE category DROP COLUMN last_update;\")\n",
    "cursor.execute(\"ALTER TABLE city DROP COLUMN last_update;\")\n",
    "cursor.execute(\"ALTER TABLE country DROP COLUMN last_update;\")\n",
    "cursor.execute(\"ALTER TABLE customer DROP COLUMN last_update;\")\n",
    "cursor.execute(\"ALTER TABLE film DROP COLUMN last_update;\")\n",
    "cursor.execute(\"ALTER TABLE film_actor DROP COLUMN last_update;\")\n",
    "cursor.execute(\"ALTER TABLE film_category DROP COLUMN last_update;\")\n",
    "cursor.execute(\"ALTER TABLE inventory DROP COLUMN last_update;\")\n",
    "cursor.execute(\"ALTER TABLE language DROP COLUMN last_update;\")\n",
    "cursor.execute(\"ALTER TABLE payment DROP COLUMN last_update;\")\n",
    "cursor.execute(\"ALTER TABLE rental DROP COLUMN last_update;\")\n",
    "cursor.execute(\"ALTER TABLE staff DROP COLUMN last_update;\")\n",
    "cursor.execute(\"ALTER TABLE store DROP COLUMN last_update;\")\n",
    "cursor.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0e130692-f9b3-4362-a423-97b5f8b3af27",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "I then executed the queries in sakila-schema.sql and sakila-data.sql using MySQLWorkbench. I also created my destination database: sakila_fact. I used the script provided to create a dim_date table and populate it with dates from 1/1/2001 to 12/31/2024.\n",
    "I now need to edit all of my tables that include date information (customer, payment, and rental) to store dim_date foreign keys rather than date objects. I decided to use pandas dataframes to do this rather than sql."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "adc3432a-35cd-47c3-87fb-ee7ec6ec9cd6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_dataframe(user_id, pwd, host_name, db_name, sql_query):\n",
    "    conn_str = f\"mysql+pymysql://{user_id}:{pwd}@{host_name}/{db_name}\"\n",
    "    sqlEngine = create_engine(conn_str, pool_recycle=3600)\n",
    "    connection = sqlEngine.connect()\n",
    "    dframe = pd.read_sql(text(sql_query), connection)\n",
    "    connection.close()\n",
    "\n",
    "    return dframe\n",
    "\n",
    "customer_df = get_dataframe(mysql_user_id, mysql_pwd, mysql_host_name, src_dbname, \"SELECT * FROM sakila.customer;\")\n",
    "payment_df = get_dataframe(mysql_user_id, mysql_pwd, mysql_host_name, src_dbname, \"SELECT * FROM sakila.payment;\")\n",
    "rental_df = get_dataframe(mysql_user_id, mysql_pwd, mysql_host_name, src_dbname, \"SELECT * FROM sakila.rental;\")\n",
    "\n",
    "dim_date = get_dataframe(mysql_user_id, mysql_pwd, mysql_host_name, src_dbname, \"SELECT * FROM sakila.dim_date;\")\n",
    "dim_date['full_date'] = dim_date['full_date'].astype(str)\n",
    "\n",
    "customer_df['create_date'] = customer_df['create_date'].astype(str).str[:10]\n",
    "customer_date = customer_df.merge(dim_date, left_on='create_date', right_on='full_date')\n",
    "customer_df = customer_df.drop(columns=['create_date'])\n",
    "customer_df['create_date_id'] = customer_date['date_key']\n",
    "\n",
    "payment_df['payment_date'] = payment_df['payment_date'].astype(str).str[:10]\n",
    "payment_date = payment_df.merge(dim_date, left_on='payment_date', right_on='full_date')\n",
    "payment_df = payment_df.drop(columns=['payment_date'])\n",
    "payment_df['payment_date_id'] = payment_date['date_key']\n",
    "\n",
    "rental_df['rental_date'] = rental_df['rental_date'].astype(str).str[:10]\n",
    "rental_date = rental_df.merge(dim_date, left_on='rental_date', right_on='full_date')\n",
    "rental_df = rental_df.drop(columns=['rental_date', 'return_date'])\n",
    "rental_df['rental_date_id'] = rental_date['date_key']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "88bd1a1d-46e2-409b-9864-319a0196242f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Now we will convert these pandas dataframes to spark dataframes, and save them as tables in our db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "297d4034-a50d-49bf-a812-b4d42f5acaad",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "s_customer_df = spark.createDataFrame(customer_df)\n",
    "s_payment_df = spark.createDataFrame(payment_df)\n",
    "s_rental_df = spark.createDataFrame(rental_df)\n",
    "s_dim_date_df = spark.createDataFrame(dim_date)\n",
    "\n",
    "s_customer_df.write.saveAsTable(\"sakila_fact.dim_customer\")\n",
    "s_payment_df.write.saveAsTable(\"sakila_fact.dim_payment\")\n",
    "s_rental_df.write.saveAsTable(\"sakila_fact.dim_rental\")\n",
    "s_dim_date_df.write.saveAsTable(\"sakila_fact.dim_date\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eddeec36-e49e-451a-989c-82a4f5a176e2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Next we will extract the tables related to information about the store involved in a transaction. This includes the tables: ‘address’, ‘city’, ‘country’, and ‘store’. We will then store the contents of these tables in a MongoDB database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8d2a7b15-182a-4493-b733-f336b0652c68",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pymongo.server_api import ServerApi\n",
    "\n",
    "address_df = get_dataframe(mysql_user_id, mysql_pwd, mysql_host_name, src_dbname, \"SELECT * FROM sakila.address;\")\n",
    "address_df = address_df.drop(columns=['location', 'address2'])\n",
    "\n",
    "city_df = get_dataframe(mysql_user_id, mysql_pwd, mysql_host_name, src_dbname, \"SELECT * FROM sakila.city;\")\n",
    "country_df = get_dataframe(mysql_user_id, mysql_pwd, mysql_host_name, src_dbname, \"SELECT * FROM sakila.country;\")\n",
    "store_df = get_dataframe(mysql_user_id, mysql_pwd, mysql_host_name, src_dbname, \"SELECT * FROM sakila.store;\")\n",
    "\n",
    "client = pymongo.MongoClient(conn_str['atlas'], server_api=ServerApi('1'))\n",
    "db = client['sakila']\n",
    "db['address'].insert_many(address_df.to_dict(orient='records'))\n",
    "db['city'].insert_many(city_df.to_dict(orient='records'))\n",
    "db['country'].insert_many(country_df.to_dict(orient='records'))\n",
    "db['store'].insert_many(store_df.to_dict(orient='records'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d04cc387-476c-4e5a-a57c-3ff063fc0290",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "s_city_df = spark.createDataFrame(city_df)\n",
    "s_country_df = spark.createDataFrame(country_df)\n",
    "s_store_df = spark.createDataFrame(store_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d6f61e2a-fbb7-4565-a587-1654ceedcab7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Extract the data back from mongodb and add it to our database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9fd969d4-ce41-4602-bbb7-5ee9f26e3315",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "mongo_uri = \"mongodb+srv://root:kinginthenorth@ethancluster.tnalhre.mongodb.net/sakila.address?retryWrites=true&w=majority\"\n",
    "\n",
    "df = spark.read.format(\"mongo\").option(\"uri\",mongo_uri).load()\n",
    "\n",
    "df.write.saveAsTable(\"sakila_fact.dim_address\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e86f4dd8-d0a8-451a-b558-695b5c947a35",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "mongo_uri = \"mongodb+srv://root:kinginthenorth@ethancluster.tnalhre.mongodb.net/sakila.city?retryWrites=true&w=majority\"\n",
    "\n",
    "df = spark.read.format(\"mongo\").option(\"uri\",mongo_uri).load()\n",
    "\n",
    "df.write.saveAsTable(\"sakila_fact.dim_city\")\n",
    "\n",
    "mongo_uri = \"mongodb+srv://root:kinginthenorth@ethancluster.tnalhre.mongodb.net/sakila.country?retryWrites=true&w=majority\"\n",
    "\n",
    "df = spark.read.format(\"mongo\").option(\"uri\",mongo_uri).load()\n",
    "\n",
    "df.write.saveAsTable(\"sakila_fact.dim_country\")\n",
    "\n",
    "mongo_uri = \"mongodb+srv://root:kinginthenorth@ethancluster.tnalhre.mongodb.net/sakila.store?retryWrites=true&w=majority\"\n",
    "\n",
    "df = spark.read.format(\"mongo\").option(\"uri\",mongo_uri).load()\n",
    "\n",
    "df.write.saveAsTable(\"sakila_fact.dim_store\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f270ba50-c33a-4e8e-a150-fa4b791d2d4a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Now I will extract the rental table and store the data in a csv file in a file called rental.csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "21f37e0a-88a0-483a-b917-7c64c2f9405c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "s_rental_df.write.format(\"csv\").option(\"header\",\"true\").save(\"/FileStore/mydata/rental.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e824ec58-b3d4-4251-ab18-63be25008b31",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Lets set up a stream for this csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9c8cf270-54cb-45c4-bb68-4dcd16bda47f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define schema\n",
    "schema = StructType([\n",
    "    StructField(\"rental_id\", IntegerType(), True),\n",
    "    StructField(\"rental_date_id\", IntegerType(), True),\n",
    "    StructField(\"inventory_id\", IntegerType(), True),  # Assuming mediumint fits within IntegerType\n",
    "    StructField(\"customer_id\", ShortType(), True),  # smallint maps to ShortType in PySpark\n",
    "    StructField(\"staff_id\", ByteType(), True),  # tinyint maps to ByteType in PySpark\n",
    "])\n",
    "\n",
    "df_stream = spark \\\n",
    "    .readStream \\\n",
    "    .option(\"maxFilesPerTrigger\", 1) \\\n",
    "    .schema(schema) \\\n",
    "    .csv(\"/FileStore/mydata/rental.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d7cd4a33-7538-46b1-bc0b-eda7864da468",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Merge all other dataframes into our streaming dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "28015e2b-8a88-4d84-a0e9-34eab03566de",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "s_inventory_df = spark.createDataFrame(get_dataframe(mysql_user_id, mysql_pwd, mysql_host_name, src_dbname, \"SELECT * FROM sakila.inventory;\"))\n",
    "s_film_df = spark.createDataFrame(get_dataframe(mysql_user_id, mysql_pwd, mysql_host_name, src_dbname, \"SELECT * FROM sakila.film;\"))\n",
    "s_staff_df = spark.createDataFrame(get_dataframe(mysql_user_id, mysql_pwd, mysql_host_name, src_dbname, \"SELECT * FROM sakila.staff;\"))\n",
    "\n",
    "fact_rentals = df_stream.join(s_payment_df, on=['rental_id', 'staff_id', 'customer_id']).drop(\"payment_id\")\n",
    "fact_rentals = fact_rentals.join(s_inventory_df, on='inventory_id').drop(\"inventory_id\")\n",
    "fact_rentals = fact_rentals.join(s_film_df, on='film_id').drop(\"description\", \"release_year\", \"language_id\", \"original_language_id\", \"rental_duration\", \"rental_rate\", \"length\", \"replacement_cost\", \"rating\", \"special_features\").withColumnRenamed(\"title\", \"film_title\")\n",
    "fact_rentals = fact_rentals.join(s_customer_df, on=['customer_id', 'store_id']).drop(\"first_name\", \"active\", \"create_date_id\").withColumnRenamed(\"email\", \"customer_email\").withColumnRenamed(\"address_id\", \"customer_address_id\").withColumnRenamed(\"last_name\", \"customer_last_name\")\n",
    "fact_rentals = fact_rentals.join(s_staff_df, on=['staff_id', 'store_id']).drop(\"address_id\", \"email\", \"first_name\").withColumnRenamed(\"last_name\", \"staff_last_name\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "21a388c4-7b48-4a3f-8125-d9934679926c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Save our streaming dataframe to the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f5b749cd-d267-4d32-88f3-c69eafed1954",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "query = fact_rentals \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"memory\") \\\n",
    "    .queryName(\"fact_rentals_table\") \\\n",
    "    .start()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 2656212829883050,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "DS 2002 Captsone",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
